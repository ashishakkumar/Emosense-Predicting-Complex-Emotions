{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install mplcyberpunk","metadata":{"execution":{"iopub.status.busy":"2023-05-14T09:08:58.093108Z","iopub.execute_input":"2023-05-14T09:08:58.093803Z","iopub.status.idle":"2023-05-14T09:09:08.323921Z","shell.execute_reply.started":"2023-05-14T09:08:58.093773Z","shell.execute_reply":"2023-05-14T09:09:08.322624Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting mplcyberpunk\n  Downloading mplcyberpunk-0.7.0-py3-none-any.whl (6.3 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mplcyberpunk) (3.6.3)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mplcyberpunk) (3.0.9)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mplcyberpunk) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mplcyberpunk) (21.3)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mplcyberpunk) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mplcyberpunk) (9.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mplcyberpunk) (1.0.7)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mplcyberpunk) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mplcyberpunk) (4.39.3)\nRequirement already satisfied: numpy>=1.19 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mplcyberpunk) (1.23.5)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mplcyberpunk) (1.16.0)\nInstalling collected packages: mplcyberpunk\nSuccessfully installed mplcyberpunk-0.7.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport pickle\n\nimport numpy as np\nimport pandas as pd\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport mplcyberpunk as mlp\nplt.style.use(\"cyberpunk\")\n\nimport nltk\nnltk.download('punkt')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-14T09:09:51.167902Z","iopub.execute_input":"2023-05-14T09:09:51.168255Z","iopub.status.idle":"2023-05-14T09:09:52.763038Z","shell.execute_reply.started":"2023-05-14T09:09:51.168228Z","shell.execute_reply":"2023-05-14T09:09:52.761746Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"def INFO(df):\n    import numpy as np # linear algebra\n    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n    info = []\n\n    for col in df.columns:\n        count_rows = len(df[col])\n        NAN_values = df[col].isna().sum()\n        percent = (NAN_values / count_rows) * 100\n        data_type = type(df[col][0])\n        col_type = df[col].dtype\n        if col_type not in [int,float]:\n            column_type = \"Categorical\"\n            Max = \"Not Applicable\"\n            Min = \"Not Applicable\"\n        else:\n            column_type = \"Numirical\"\n            Max = max(df[col])\n            Min = min(df[col])\n        try:\n            n_uniques =df[col].nunique()\n            ratio = count_rows/n_uniques\n        except:\n            n_uniques = \"Not Applicable\"\n            ratio = \"Not Applicable\"\n        info.append([col,data_type,column_type, count_rows, NAN_values, percent,n_uniques,ratio,Max, Min])\n\n    col_info_df = pd.DataFrame(info, columns=['Column','Data Type','Column Type', 'count_rows', 'Missing', 'Percent Missing','Number of Uniques','Ratio of uniqus','Max','Min'])\n\n    return col_info_df","metadata":{"execution":{"iopub.status.busy":"2023-05-14T09:09:52.764605Z","iopub.execute_input":"2023-05-14T09:09:52.764864Z","iopub.status.idle":"2023-05-14T09:09:52.772101Z","shell.execute_reply.started":"2023-05-14T09:09:52.764840Z","shell.execute_reply":"2023-05-14T09:09:52.771172Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndef classify_emotions(v_valence, v_arousal, v_dominance):\n    \"\"\"\n    Classify emotions based on valence, arousal, and dominance values.\n\n    Parameters:\n        v_valence (float): Valence value of the input point.\n        v_arousal (float): Arousal value of the input point.\n        v_dominance (float): Dominance value of the input point.\n\n    Returns:\n        None. Prints the closest emotions with their intensities.\n    \"\"\"\n    # Load the dataset from a CSV file\n    dataset = pd.read_csv(\"https://raw.githubusercontent.com/karthik0899/EmoSense/main/VAD_values.csv\")\n    point = np.array([v_valence, v_arousal, v_dominance])\n    \n    # Extract the means and standard deviations for each emotion\n    emotion_names = dataset['Emotion'].unique()\n    means = np.zeros((len(emotion_names), 3))\n    stds = np.zeros((len(emotion_names), 3))\n    for i, emotion in enumerate(emotion_names):\n        sub_df = dataset[dataset['Emotion'] == emotion]\n        means[i] = sub_df[['V_MEAN', 'A_MEAN', 'D_MEAN']].values\n        stds[i] = sub_df[['V_SD', 'A_SD', 'D_SD']].values\n\n    # Calculate the distances between the point and the centers of each ellipsoid\n    distances = []\n    for i in range(len(means)):\n        center = means[i]\n        distance = np.linalg.norm(point - center)\n        distances.append(distance)\n\n    # Sort the emotions by distance and select the top 5\n    sorted_idx = np.argsort(distances)\n    top5_idx = sorted_idx[:5]\n    top5_emotions = emotion_names[top5_idx]\n\n    # Calculate the intensities of the top 5 emotions in terms of a percentage\n    max_distance = np.max(distances)\n    intensities = []\n    for i in top5_idx:\n        intensities.append((1 - distances[i] / max_distance) * 100)\n\n    # Print the result\n    print(\"The point [Valence=\", v_valence, \", Arousal=\", v_arousal, \", Dominance=\", v_dominance, \"] closely resembles the following emotions with the following intensities:\")\n    for i, emotion in enumerate(top5_emotions):\n        print(emotion, \":\", intensities[i], \"%\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-14T09:09:52.773242Z","iopub.execute_input":"2023-05-14T09:09:52.773549Z","iopub.status.idle":"2023-05-14T09:09:52.786848Z","shell.execute_reply.started":"2023-05-14T09:09:52.773521Z","shell.execute_reply":"2023-05-14T09:09:52.785997Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef range_scaler(array, assumed_max_input=5, assumed_min_input=1):\n    \"\"\"\n    Scale the values in the input array to the range of -1 to 1.\n\n    Parameters:\n        array (ndarray): Input array to be scaled.\n        assumed_max_input (float): Assumed maximum value of the input array. Default is 5.\n        assumed_min_input (float): Assumed minimum value of the input array. Default is 1.\n\n    Returns:\n        ndarray: Scaled array with values ranging from -1 to 1.\n    \"\"\"\n    array_std = (array - assumed_min_input) / (assumed_max_input - assumed_min_input)\n    array_scaled = array_std * (1 - (-1)) + (-1)\n    return array_scaled\n","metadata":{"execution":{"iopub.status.busy":"2023-05-14T09:09:52.789195Z","iopub.execute_input":"2023-05-14T09:09:52.789539Z","iopub.status.idle":"2023-05-14T09:09:52.803136Z","shell.execute_reply.started":"2023-05-14T09:09:52.789511Z","shell.execute_reply":"2023-05-14T09:09:52.802241Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def preprocess_dataframe(df):\n    \n    df = df[df['text'].str.split().apply(len) >= 4].copy() # Drop rows with less than 4 words\n    \n    # Convert all words to lowercase and remove HTML tags\n    df['text'] = df['text'].str.lower().str.replace('<br /><br />', ' ')\n    \n    # Replace multiple spaces with a single space\n    df['text'] = df['text'].str.replace('\\s+', ' ', regex=True)\n    \n    # Drop rows with only numerical values\n    df = df[~df['text'].str.isnumeric()].copy()\n    \n    return df\n\n\n\ndef plot_VAD_histograms(df):\n    \n    fig, axs = plt.subplots(1, 3, figsize=(15, 6))\n    axs[0].hist(df['V'], color='red')\n    axs[0].set_xlabel('Valence')\n    axs[0].set_ylabel('Frequency')\n    axs[1].hist(df['A'], color='green')\n    axs[1].set_xlabel('Arousal')\n    axs[2].hist(df['D'], color='blue')\n    axs[2].set_xlabel('Dominance')\n    fig.suptitle('Frequency of VAD Values')\n    plt.show()\n    \n    return df\n\n\n\ndef plot_VAD_boxplot(data):\n    \"\"\"\n    Plots a boxplot of the VAD (Valence, Arousal, Dominance) values in the given dataframe.\n\n    Parameters:\n    data (pandas.DataFrame): A pandas DataFrame containing columns for V, A, and D values.\n\n    Returns:\n    None\n    \"\"\"\n    plt.figure(figsize=(15,6))\n    sns.boxplot(data=data[['V', 'A', 'D']], palette='Set3')\n    plt.title('Distribution of VAD Values')\n    plt.legend(labels=['Valence (V)', 'Arousal (A)', 'Dominance (D)'])\n    plt.show()\n    return df\n\n\n\ndef plot_vad_scatter(df):\n    \n    plt.figure(figsize=(15,6))\n    sns.scatterplot(x='V', y='A', hue='D', data=df)  \n    plt.title(\"Scatter Plot of VAD Values\")\n    plt.xlabel(\"Valence (V)\")\n    plt.ylabel(\"Arousal (A)\")\n    \n    plt.legend(title=\"Dominance (D)\", loc='best', labels=['Low', 'Medium', 'High'])\n    \n    plt.show()\n    return df\n\n    \n\n\n\ndef plot_emotion_distributions(preprocessed_df):\n    \n    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(18,6))\n    \n    sns.distplot(df['V'], ax=axs[0], color='red')\n    axs[0].set_xlabel('Valence')\n    axs[0].set_ylabel('Density')\n    \n    sns.distplot(df['A'], ax=axs[1], color='green')\n    axs[1].set_xlabel('Arousal')\n    axs[1].set_ylabel('Density')\n    \n    sns.distplot(df['D'], ax=axs[2], color='blue')\n    axs[2].set_xlabel('Dominance')\n    axs[2].set_ylabel('Density')\n    \n    plt.show()\n    return df\n\n    \n\n    \ndef plot_VAD_pairplot(df):\n    \n    sns.pairplot(df, vars=['V', 'A', 'D'],height = 4)\n    return df\n\n\n    \ndef create_heatmap(df):\n\n    df['word_counts'] = df['text'].apply(lambda x: len(x.split()))     # Add word count column to the DataFrame\n\n    # Add special character count column to the DataFrame\n    df['special_chars_count'] = df['text'].apply(lambda x: len(re.findall(r'[^\\w\\s]', x)))\n\n    # Compute the correlation matrix\n    corr_matrix = df[['V', 'A', 'D','word_counts','special_chars_count']].corr()\n\n    # Set up the figure\n    fig, ax = plt.subplots(figsize=(8, 6))\n\n    # Draw the heatmap\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0, ax=ax)\n\n    # Set the title\n    ax.set_title('Correlation Heat Map between the Features')\n\n    return df\n\n\n\ndef plot_top_words_and_special_chars(df, num_words=30, num_chars=30):\n    # Plot the top most occurring words and their frequencies\n    top_words = df['text'].str.split(expand=True).stack().value_counts()[:num_words]\n    plt.figure(figsize=(15,8))\n    sns.barplot(x=top_words.index, y=top_words.values, alpha=0.8)\n    plt.title('Top {} Most Occurring Words'.format(num_words))\n    plt.ylabel('Frequency', fontsize=12)\n    plt.xlabel('Words', fontsize=12)\n    plt.xticks(rotation=45)\n\n    # Plot the top most occurring special characters and their frequencies\n    top_special_chars = df['text'].str.findall(r'[^\\w\\s]').explode().value_counts()[:num_chars]\n    plt.figure(figsize=(15,8))\n    sns.barplot(x=top_special_chars.index, y=top_special_chars.values, alpha=0.8)\n    plt.title('Top {} Most Occurring Special Characters'.format(num_chars))\n    plt.ylabel('Frequency', fontsize=12)\n    plt.xlabel('Special Characters', fontsize=12)\n\n    return plt.show()\n\n\n\ndef plot_VAD_vs_special_chars_scatter(df):\n    \n    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,6))\n\n    axs[0].scatter(df['special_chars_count'], df['V'], color='red', label='Valence')\n    axs[0].set_xlabel('Count of Special Characters')\n    axs[0].set_ylabel('Valence')\n    axs[0].legend()\n\n    axs[1].scatter(df['special_chars_count'], df['A'], color='green', label='Arousal')\n    axs[1].set_xlabel('Count of Special Characters')\n    axs[1].set_ylabel('Arousal')\n    axs[1].legend()\n\n    axs[2].scatter(df['special_chars_count'], df['D'], color='blue', label='Dominance')\n    axs[2].set_xlabel('Count of Special Characters')\n    axs[2].set_ylabel('Dominance')\n    axs[2].legend()\n\n    plt.show()\n    return df\n\n\n    \ndef plot_VAD_vs_WordCounts(df):\n\n    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,6))\n\n    axs[0].scatter(df['word_counts'], df['V'], color='red', label='Valence')\n    axs[0].set_xlabel('Word Counts')\n    axs[0].set_ylabel('Valence')\n    axs[0].legend()\n\n    axs[1].scatter(df['word_counts'], df['A'], color='green', label='Arousal')\n    axs[1].set_xlabel('Word Counts')\n    axs[1].set_ylabel('Arousal')\n    axs[1].legend()\n\n    axs[2].scatter(df['word_counts'], df['D'], color='blue', label='Dominance')\n    axs[2].set_xlabel('Word Counts')\n    axs[2].set_ylabel('Dominance')\n    axs[2].legend()\n\n    plt.show()\n    return df\n\n\n    \ndef plot_word_counts_frequencies(df):\n    \n    plt.figure(figsize=(15,6))\n    sns.histplot(df['word_counts'], kde=False)\n    plt.title(\"Word Counts Vs Frequency\")\n    plt.xlabel(\"Word Counts\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n    return df\n\n    \n\n\ndef plot_word_cloud(data_frame, column_name, width=1500, height=1000, max_words=200):\n    \"\"\"\n    Plots a word cloud of the most frequent words in a given column of a Pandas DataFrame.\n\n    Parameters:\n    data_frame (pandas.DataFrame): The DataFrame containing the text column.\n    column_name (str): The name of the text column to be analyzed.\n    width (int, optional): Width of the word cloud plot. Default is 1500.\n    height (int, optional): Height of the word cloud plot. Default is 1000.\n    max_words (int, optional): Maximum number of words to include in the word cloud plot. Default is 200.\n    \"\"\"\n\n    text = ' '.join(data_frame[column_name])\n    wordcloud = WordCloud(width=width, height=height, background_color='white', max_words=max_words).generate(text)\n    plt.figure(figsize=(12,6), facecolor=None)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad=0)\n    plt.show()\n    return df\n\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-05-14T09:09:52.804301Z","iopub.execute_input":"2023-05-14T09:09:52.804611Z","iopub.status.idle":"2023-05-14T09:09:52.832329Z","shell.execute_reply.started":"2023-05-14T09:09:52.804582Z","shell.execute_reply":"2023-05-14T09:09:52.831233Z"},"trusted":true},"execution_count":7,"outputs":[]}]}